{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cholesterol-lowering Supplement Classification\n",
    "\n",
    "This notebook implements a machine learning model to predict if a patient needs cholesterol-lowering supplements based on their health attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "training_data = pd.read_excel(\"data/train_choloesterol.xlsx\")\n",
    "\n",
    "# Load the prediction data\n",
    "prediction_data = pd.read_excel(\"data/predict_cholesterol.xlsx\")\n",
    "\n",
    "# Display first few rows of training data\n",
    "print(\"Training data shape:\", training_data.shape)\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows of prediction data\n",
    "print(\"Prediction data shape:\", prediction_data.shape)\n",
    "prediction_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in training data\n",
    "print(\"Missing values in training data:\")\n",
    "print(training_data.isnull().sum())\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nData types in training data:\")\n",
    "print(training_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistical summary of training data\n",
    "training_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of target variable\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x=\"Need Supplement\", data=training_data)\n",
    "plt.title(\"Distribution of Target Variable\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Target variable distribution:\")\n",
    "print(training_data[\"Need Supplement\"].value_counts())\n",
    "print(\"Percentage:\")\n",
    "print(training_data[\"Need Supplement\"].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore categorical variables\n",
    "categorical_features = [\n",
    "    \"Gender\",\n",
    "    \"Physical Activity\",\n",
    "    \"Dietary Habits\",\n",
    "    \"Family History\",\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(categorical_features):\n",
    "    sns.countplot(x=feature, hue=\"Need Supplement\", data=training_data, ax=axes[i])\n",
    "    axes[i].set_title(f\"Distribution of {feature} by Need Supplement\")\n",
    "    axes[i].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore numerical variables\n",
    "numerical_features = [\n",
    "    \"Age\",\n",
    "    \"BMI\",\n",
    "    \"Total Cholesterol\",\n",
    "    \"LDL Cholesterol\",\n",
    "    \"HDL Cholesterol\",\n",
    "    \"Triglycerides\",\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(numerical_features):\n",
    "    sns.boxplot(x=\"Need Supplement\", y=feature, data=training_data, ax=axes[i])\n",
    "    axes[i].set_title(f\"Distribution of {feature} by Need Supplement\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numerical features\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = training_data[numerical_features + [\"Need Supplement\"]].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable for training data\n",
    "X = training_data.drop(\"Need Supplement\", axis=1)\n",
    "y = training_data[\"Need Supplement\"]\n",
    "\n",
    "# Define categorical and numerical features\n",
    "categorical_features = [\n",
    "    \"Gender\",\n",
    "    \"Physical Activity\",\n",
    "    \"Dietary Habits\",\n",
    "    \"Family History\",\n",
    "]\n",
    "numerical_features = [\n",
    "    \"Age\",\n",
    "    \"BMI\",\n",
    "    \"Total Cholesterol\",\n",
    "    \"LDL Cholesterol\",\n",
    "    \"HDL Cholesterol\",\n",
    "    \"Triglycerides\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipeline\n",
    "# For numerical features: StandardScaler\n",
    "# For categorical features: OneHotEncoder\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numerical_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Building and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Random Forest Pipeline\n",
    "rf_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", RandomForestClassifier(random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Parameter grid for Random Forest\n",
    "rf_param_grid = {\n",
    "    \"classifier__n_estimators\": [100, 200],\n",
    "    \"classifier__max_depth\": [None, 10, 20],\n",
    "    \"classifier__min_samples_split\": [2, 5, 10],\n",
    "    \"classifier__min_samples_leaf\": [1, 2, 4],\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "rf_grid_search = GridSearchCV(\n",
    "    rf_pipeline, rf_param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters for Random Forest:\")\n",
    "print(rf_grid_search.best_params_)\n",
    "\n",
    "# Best estimator\n",
    "rf_best = rf_grid_search.best_estimator_\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_rf = rf_best.predict(X_test)\n",
    "print(\"\\nRandom Forest Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gradient Boosting Pipeline\n",
    "gb_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", GradientBoostingClassifier(random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Parameter grid for Gradient Boosting\n",
    "gb_param_grid = {\n",
    "    \"classifier__n_estimators\": [100, 200],\n",
    "    \"classifier__learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"classifier__max_depth\": [3, 5, 7],\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "gb_grid_search = GridSearchCV(\n",
    "    gb_pipeline, gb_param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1\n",
    ")\n",
    "\n",
    "gb_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters for Gradient Boosting:\")\n",
    "print(gb_grid_search.best_params_)\n",
    "\n",
    "# Best estimator\n",
    "gb_best = gb_grid_search.best_estimator_\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_gb = gb_best.predict(X_test)\n",
    "print(\"\\nGradient Boosting Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_gb):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Logistic Regression Pipeline\n",
    "lr_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", LogisticRegression(random_state=42, max_iter=1000)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Parameter grid for Logistic Regression\n",
    "lr_param_grid = {\n",
    "    \"classifier__C\": [0.01, 0.1, 1, 10, 100],\n",
    "    \"classifier__solver\": [\"liblinear\", \"saga\"],\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "lr_grid_search = GridSearchCV(\n",
    "    lr_pipeline, lr_param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1\n",
    ")\n",
    "\n",
    "lr_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters for Logistic Regression:\")\n",
    "print(lr_grid_search.best_params_)\n",
    "\n",
    "# Best estimator\n",
    "lr_best = lr_grid_search.best_estimator_\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_lr = lr_best.predict(X_test)\n",
    "print(\"\\nLogistic Regression Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SVM Pipeline\n",
    "svm_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", SVC(random_state=42, probability=True)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Parameter grid for SVM\n",
    "svm_param_grid = {\n",
    "    \"classifier__C\": [0.1, 1, 10],\n",
    "    \"classifier__kernel\": [\"linear\", \"rbf\"],\n",
    "    \"classifier__gamma\": [\"scale\", \"auto\"],\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "svm_grid_search = GridSearchCV(\n",
    "    svm_pipeline, svm_param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1\n",
    ")\n",
    "\n",
    "svm_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters for SVM:\")\n",
    "print(svm_grid_search.best_params_)\n",
    "\n",
    "# Best estimator\n",
    "svm_best = svm_grid_search.best_estimator_\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_svm = svm_best.predict(X_test)\n",
    "print(\"\\nSVM Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_svm):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "models = {\n",
    "    \"Random Forest\": rf_best,\n",
    "    \"Gradient Boosting\": gb_best,\n",
    "    \"Logistic Regression\": lr_best,\n",
    "    \"SVM\": svm_best,\n",
    "}\n",
    "\n",
    "# Cross-validation results\n",
    "cv_results = {}\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring=\"accuracy\")\n",
    "    cv_results[name] = scores\n",
    "    print(f\"{name} - Mean CV Accuracy: {scores.mean():.4f} (±{scores.std():.4f})\")\n",
    "\n",
    "# Visualize model comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "box_data = [cv_results[model_name] for model_name in models.keys()]\n",
    "plt.boxplot(box_data, labels=list(models.keys()))\n",
    "plt.title(\"Model Comparison - Cross-Validation Accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC = {roc_auc:.2f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Random\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model based on cross-validation results\n",
    "best_model_name = max(cv_results, key=lambda k: cv_results[k].mean())\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"The best model is: {best_model_name}\")\n",
    "\n",
    "# Feature importance for the best model (if applicable)\n",
    "if best_model_name in [\"Random Forest\", \"Gradient Boosting\"]:\n",
    "    # Get feature names after preprocessing\n",
    "    preprocessor_output_feature_names = numerical_features + list(\n",
    "        best_model.named_steps[\"preprocessor\"]\n",
    "        .transformers_[1][1]  # OneHotEncoder\n",
    "        .get_feature_names_out(categorical_features)\n",
    "    )\n",
    "\n",
    "    # Get feature importances\n",
    "    importances = best_model.named_steps[\"classifier\"].feature_importances_\n",
    "\n",
    "    # Create a DataFrame for better visualization\n",
    "    feature_importance_df = pd.DataFrame(\n",
    "        {\"Feature\": preprocessor_output_feature_names, \"Importance\": importances}\n",
    "    ).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "    # Display top 15 features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x=\"Importance\", y=\"Feature\", data=feature_importance_df.head(15))\n",
    "    plt.title(f\"Top 15 Feature Importances - {best_model_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Make Predictions on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the prediction data\n",
    "prediction_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the best model\n",
    "predictions = best_model.predict(prediction_data)\n",
    "prediction_probabilities = best_model.predict_proba(prediction_data)[:, 1]\n",
    "\n",
    "# Add predictions to the prediction data\n",
    "prediction_results = prediction_data.copy()\n",
    "prediction_results[\"Need Supplement\"] = predictions\n",
    "prediction_results[\"Probability\"] = prediction_probabilities\n",
    "\n",
    "# Display the results\n",
    "print(\"Predictions for the 20 patients:\")\n",
    "display(prediction_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of predictions\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x=\"Need Supplement\", data=prediction_results)\n",
    "plt.title(\"Distribution of Predictions\")\n",
    "plt.xticks([0, 1], [\"No Need for Supplement\", \"Need Supplement\"])\n",
    "plt.show()\n",
    "\n",
    "print(\"Prediction distribution:\")\n",
    "print(prediction_results[\"Need Supplement\"].value_counts())\n",
    "print(\"Percentage:\")\n",
    "print(prediction_results[\"Need Supplement\"].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions to Excel file\n",
    "prediction_results.to_excel(\"predict_cholesterol_surname_name.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "In this project, we developed a machine learning classification model to predict whether a patient needs a cholesterol-lowering supplement based on various health attributes.\n",
    "\n",
    "Key findings:\n",
    "1. We explored the data and found relationships between health metrics and the need for supplements\n",
    "2. We built and compared four different machine learning models:\n",
    "   - Random Forest\n",
    "   - Gradient Boosting\n",
    "   - Logistic Regression\n",
    "   - Support Vector Machine\n",
    "3. The best performing model was selected based on cross-validation results\n",
    "4. We applied this model to predict supplement needs for 20 new patients\n",
    "\n",
    "The most influential features in determining whether a patient needs a supplement were likely cholesterol levels (particularly LDL and Total Cholesterol), along with other risk factors like age, BMI, and family history.\n",
    "\n",
    "This model can be used as a decision support tool for healthcare providers when determining if a patient would benefit from cholesterol-lowering supplements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}